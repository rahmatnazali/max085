On LiveMode
--------------------------------------------------------------------------------------------------------------------------------------------------------------
This issue only happens when GenerateNewFiles=False

Removing Rows With Same Column # Values Except 1st Occurrence In Output.csv ...
||
\/


For example: Column=Links with GenerateNewFiles=False , the scaraped date is:

	Title ABC , https://example.com/x/1000/
	Title XYZ , https://example.com/x/2500/
	Title HIJ , https://example.com/c/5000/


If I run the software it will store these in output.csv which is correct:
	Title ABC , https://example.com/x/1000/
	Title XYZ , https://example.com/x/2500/
	Title HIJ , https://example.com/c/5000/
	

If I run the software again with the same config, the output.csv would result like this which is not correct.:
	Title ABC , https://example.com/x/1000/
	Title XYZ , https://example.com/x/2500/
	Title HIJ , https://example.com/c/5000/
	Title ABC , https://example.com/x/1000/
	Title XYZ , https://example.com/x/2500/
	Title HIJ , https://example.com/c/5000/
	
	
	notice that Lines: 27,28,29 are duplicated
	
	solving it I think would be after storing the results in output.csv then make the comparisons on output.csv to remove the Link Column duplicates
	as it's needs also to compare old data already in output.csv from previous runs with the new added data.
		
--------------------------------------------------------------------------------------------------------------------------------------------------------------
	
*Adding Results to Output.csv ...  (20 row(s) of data found) 									   -> this works great
*Regex Search & Replace in Output.csv ... (4/4 Regex Matched) 									   -> this appears (4/4 Regex Matched) always
*Removing Rows In Output.csv If The Row Also In Done.csv ... (0 Rows Removed) 					   -> this appears (0 Rows Removed) always
*Removing Rows With Same Column # Values Except 1st Occurrence In Output.csv ... (0 Rows Removed)  -> this appears (0 Rows Removed) always

Everything else I tested seems to be working great, thanks.

--------------------------------------------------------------------------------------------------------------------------------------------------------------


May I also add this request to increase efficiency:

* For any url in URLs.txt that is successfully scraped and its results added to output.csv, it is Removed from URLs.txt and Added to URLsDone.txt, this is for in case the script shuts down or exit suddenly, when it runs again it will continue and not repeat already done URLs

* at the end of the software run print: 
	URLs Completed Logged in (URLsDone.txt): #/#TotalURLs
	Errors Logged in (Links-Errors.txt): #
	Errors Logged in (XPaths-Errors.txt): #
--------------------------------------------------------------------------------------------------------------------------------------------------------------